{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllTags(url, tag, attrs=None):\n",
    "    req = requests.get(url)\n",
    "    parser = bs4.BeautifulSoup(req.text, 'lxml')\n",
    "    return parser.findAll(tag, attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Парсинг веб-страниц\n",
    "В этом задании вы потренеруетесь парсить веб-страницы. Это умение пригодится вам, когда возникнет потребность самостоятельно собрать выборку для построения модели, будь то тексты или какая-то другая информация с общедоступных веб-страниц. В рамках данного проекта это случится уже в задании следующей недели.\n",
    "\n",
    "## Задание 1\n",
    "Изучите блокнот с примерами парсинга страниц (в аттаче)\n",
    "\n",
    "## Задание 2\n",
    "Поэкспериментируйте с парсингом любой страницы на ваш вкус, бегло просмотрите документацию библиотек beautiful soup и requests. При парсинге вам в любом случае потребуется смотреть на html-код страницы, чтобы понять, какие элементы вас интересуют.\n",
    "### Пример парсинга страницы отзывов на книгу о Ведьмаке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Одна из любимейших серий\nНаконец дождался выхода всех книг в электронном формате.\nИзначально с Ведьмаком познакомился заочно, по скупым описаниям игры, в которую играли друзья. Так, чтобы полностью владеть всей историей начал читать книги.\nВ итоге оказался в полном восторге от всего прочитанного. Первые истории – очень своеобразные сказки. Затем формируется глубокий, продуманный фентезийный мир, в котором очень серьезно, жизненно и реалистично описываются все события. А в итоге получившееся произведение можно назвать только наилюбимейшей сагой.\nПусть мне кажется, что сначала это просто небольшие несвязные истории, которые автор подает от лица одного персонажа, но в следующих книгах все становится взаимосвязанным, все действующие лица становятся невероятно интересными, живыми. Здесь можно влюбиться в характер каждого персонажа, заучивать и напевать стихи из книги \n(Любить тебя, вот жизни цель, бесспорная как смерть, прекрасная Эттариэль…)\n.\nДополнительно, хочу добавить, что сама история из книг действительно перекочевала в компьютерные игры, так что, если у кого-либо есть желание узнать продолжение, то можно обратиться к другому жанру повествований. А может автор изложит продолжение истории в новых книгах, ведь чудеса случаются…\n"
    }
   ],
   "source": [
    "url = 'https://pda.litres.ru/andzhey-sapkovskiy/vedmak/otzivi/'\n",
    "reviews = getAllTags(url, 'div', 'recenses-item__content')\n",
    "print(reviews[0].get_text(separator=\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример парсинга страниц отзывов на книги про фентези с litres\n",
    "Запускать parse_book_reviews.py в PyCharm, Visual Studio Code или терминале!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Writing parse_book_reviews.py\n"
    }
   ],
   "source": [
    "%%writefile parse_book_reviews.py\n",
    "import requests\n",
    "import bs4\n",
    "from multiprocessing import Pool\n",
    "from functools import reduce\n",
    "\n",
    "def getAllTags(url, tag, attrs):\n",
    "    req = requests.get(url)\n",
    "    parser = bs4.BeautifulSoup(req.text, 'lxml')\n",
    "    return parser.findAll(tag, attrs)\n",
    "    \n",
    "def parse_page(url):\n",
    "    divs = getAllTags(url, 'div', 'recenses-item__content')\n",
    "    return [div.get_text(separator=\"\\n\") + '\\n' for div in divs]\n",
    "    \n",
    "if __name__ == '__main__':    \n",
    "    main_url = 'https://www.litres.ru/knigi-fentezi/'\n",
    "    req = requests.get(main_url)\n",
    "    parser = bs4.BeautifulSoup(req.text, 'lxml')\n",
    "    book_tags = parser.findAll('a', attrs={'class':'img-a'})\n",
    "    books_urls = ['https://pda.litres.ru' + str(book_tag['href']) + 'otzivi/' for book_tag in book_tags]\n",
    "\n",
    "    p = Pool(8)\n",
    "    map_results = p.map(parse_page, books_urls)\n",
    "    reduce_results = reduce(lambda x,y: x + y, map_results)\n",
    "    \n",
    "    with open('parsing_results.txt','w', encoding=\"utf-8\") as file:\n",
    "        file.write('\\n'.join(reduce_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Одна из любимейших серий\nНаконец дождался выхода всех книг в электронном формате.\nИзначально с Ведьмаком познакомился заочно, по скупым описаниям игры, в которую играли друзья. Так, чтобы полностью владеть всей историей начал читать книги.\nВ итоге оказался в полном восторге от всего прочитанного. Первые истории – очень своеобразные сказки. Затем формируется глубокий, продуманный фентезийный мир, в котором очень серьезно, жизненно и реалистично описываются все события. А в итоге получившееся произведение можно назвать только наилюбимейшей сагой.\nПусть мне кажется, что сначала это просто небольшие несвязные истории, которые автор подает от лица одного персонажа, но в следующих книгах все становится взаимосвязанным, все действующие лица становятся невероятно интересными, живыми. Здесь можно влюбиться в характер каждого персонажа, заучивать и напевать стихи из книги \n(Любить тебя, вот жизни цель, бесспорная как смерть, прекрасная Эттариэль…)\n.\nДополнительно, хочу добавить, что сама история из книг действительно перекочевала в компьютерные игры, так что, если у кого-либо есть желание узнать продолжение, то можно обратиться к другому жанру повествований. А может автор изложит продолжение истории в новых книгах, ведь чудеса случаются…\n"
    }
   ],
   "source": [
    "with open('parsing_results.txt','r', encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "print(text[:1247])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3\n",
    "Чтобы продемонстрировать, что вы разобрались, как использовать requests и beautiful soup, распарсите: \n",
    "\n",
    "* из статьи https://en.wikipedia.org/wiki/Bias-variance_tradeoff все заголовки верхнего уровня; \n",
    "* со страницы https://en.wikipedia.org/wiki/Category:Machine_learning_algorithms названия всех статей в категории Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Contents',\n 'Motivation[edit]',\n 'Bias–variance decomposition of squared error[edit]',\n 'Derivation[edit]',\n 'Application to regression[edit]',\n 'Application to classification[edit]',\n 'Application to reinforcement learning[edit]',\n 'Approaches[edit]',\n 'k-nearest neighbors[edit]',\n 'Application to human learning[edit]',\n 'See also[edit]',\n 'References[edit]',\n 'Navigation menu',\n '\\nPersonal tools\\n',\n '\\nNamespaces\\n',\n '\\nVariants\\n',\n '\\nViews\\n',\n '\\nMore\\n',\n '\\nSearch\\n',\n '\\nNavigation\\n',\n '\\nContribute\\n',\n '\\nTools\\n',\n '\\nLanguages\\n',\n '\\nPrint/export\\n']"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/Bias-variance_tradeoff'\n",
    "headings = getAllTags(url, [\"h1\", \"h2\", \"h3\"])\n",
    "[heading.text for heading in headings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Adagrad',\n 'Algorithms of Oppression',\n 'Almeida–Pineda recurrent backpropagation',\n 'Backpropagation',\n 'Bioz',\n 'Bootstrap aggregating',\n 'Cancer Likelihood in Plasma',\n 'CN2 algorithm',\n 'Constructing skill trees',\n 'Deep reinforcement learning',\n 'Dehaene–Changeux model',\n 'Diffusion map',\n 'Dominance-based rough set approach',\n 'Dynamic time warping',\n 'Elastic net regularization',\n 'Error-driven learning',\n 'Evolutionary multimodal optimization',\n 'Expectation–maximization algorithm',\n 'Extremal Ensemble Learning',\n 'FastICA',\n 'Forward–backward algorithm',\n 'GeneRec',\n 'Genetic Algorithm for Rule Set Production',\n 'Growing self-organizing map',\n 'Hyper basis function network',\n 'IDistance',\n 'K-nearest neighbors algorithm',\n 'Kernel methods for vector output',\n 'Kernel principal component analysis',\n 'Label propagation algorithm',\n 'Leabra',\n 'Linde–Buzo–Gray algorithm',\n 'Local outlier factor',\n 'Logic learning machine',\n 'LogitBoost',\n 'Loss functions for classification',\n 'Manifold alignment',\n 'Minimum redundancy feature selection',\n 'Mixture of experts',\n 'Multiple kernel learning',\n 'Non-negative matrix factorization',\n 'Online machine learning',\n 'Out-of-bag error',\n 'Prefrontal cortex basal ganglia working memory',\n 'Prototype methods',\n 'PVLV',\n 'Q-learning',\n 'Quadratic unconstrained binary optimization',\n 'Query-level feature',\n 'Quickprop',\n 'Radial basis function network',\n 'Randomized weighted majority algorithm',\n 'Repeated incremental pruning to produce error reduction (RIPPER)',\n 'Rprop',\n 'Rule-based machine learning',\n 'Skill chaining',\n 'Sparse PCA',\n 'State–action–reward–state–action',\n 'Stochastic gradient descent',\n 'Structured kNN',\n 'T-distributed stochastic neighbor embedding',\n 'Triplet loss',\n 'Wake-sleep algorithm',\n 'Weighted majority algorithm (machine learning)',\n 'Zero-shot learning']"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "ml_url = 'https://en.wikipedia.org/wiki/Category:Machine_learning_algorithms'\n",
    "divs = getAllTags(ml_url, 'div', 'mw-category-group')\n",
    "algo_names = []\n",
    "for div in divs:\n",
    "    for algo_name in div.findAll('a'):\n",
    "        algo_names.append(algo_name.text)\n",
    "algo_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}